#!/usr/bin/env python3
import sqlite3
import json
import time
import os

# --- Configuration ---
# This should point to the database file generated by falcon_monitor.py
DB_FILE = "health_monitor.db"
# How often to check for new records to sync (in seconds)
SYNC_INTERVAL = 600  # 10 minutes

def add_sync_column_if_not_exists():
    """
    Checks if the 'synced_to_cloud' column exists in the metrics table
    and adds it if it doesn't. This makes the script plug-and-play.
    """
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        # Check if the column exists
        cursor.execute("PRAGMA table_info(metrics)")
        columns = [col[1] for col in cursor.fetchall()]
        if 'synced_to_cloud' not in columns:
            print("Column 'synced_to_cloud' not found. Adding it to the database...")
            # Use INTEGER 0 for False, 1 for True
            cursor.execute("ALTER TABLE metrics ADD COLUMN synced_to_cloud INTEGER DEFAULT 0")
            conn.commit()
            print("Column added successfully.")
        conn.close()
    except Exception as e:
        print(f"ERROR: Could not verify or add sync column to database. {e}")
        # Exit if we can't set up the database correctly
        exit(1)

def fetch_unsynced_data():
    """
    Connects to the SQLite DB and fetches all records that have not been synced yet.
    Returns a list of dictionaries, where each dictionary is a record.
    """
    try:
        conn = sqlite3.connect(DB_FILE)
        # This makes the cursor return rows that can be accessed by column name
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        print("Checking for new records to sync...")
        cursor.execute("SELECT * FROM metrics WHERE synced_to_cloud = 0")
        records = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        if records:
            print(f"Found {len(records)} new record(s) to sync.")
        else:
            print("No new records to sync.")
            
        return records
    except Exception as e:
        print(f"ERROR: Could not fetch unsynced data from database. {e}")
        return []

def mark_data_as_synced(records):
    """
    Updates records in the database to mark them as synced.
    """
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        # Get the timestamps (primary keys) of the records to update
        timestamps_to_update = [rec['timestamp'] for rec in records]
        
        # Use a placeholder for the IN clause to prevent SQL injection
        placeholders = ', '.join(['?'] * len(timestamps_to_update))
        query = f"UPDATE metrics SET synced_to_cloud = 1 WHERE timestamp IN ({placeholders})"
        
        cursor.execute(query, timestamps_to_update)
        conn.commit()
        conn.close()
        print(f"Successfully marked {len(records)} record(s) as synced.")
    except Exception as e:
        print(f"ERROR: Could not mark data as synced. {e}")

# ############################################################################
# ##                                                                        ##
# ##          >>>   THIS IS THE ONLY FUNCTION TO EDIT   <<<                 ##
# ##                                                                        ##
# ############################################################################

def upload_data_to_cloud(records):
    """
    This is the placeholder function where a developer adds their cloud logic.
    
    Args:
        records (list): A list of dictionaries, each representing a row from the DB.
        
    Returns:
        bool: True if the upload was successful, False otherwise.
    """
    print("Attempting to upload data to the cloud...")
    
    # Convert the list of records to a JSON string for easy uploading
    data_to_upload = json.dumps(records, indent=2)
    
    # --- CHOOSE ONE OF THE EXAMPLES BELOW AND FILL IN YOUR DETAILS ---

    # --- Example 1: Upload to a generic REST API Endpoint ---
    # try:
    #     import requests
    #     api_url = "https://your-cloud-api.com/endpoint"
    #     headers = {"Content-Type": "application/json", "Authorization": "Bearer YOUR_API_KEY"}
    #     response = requests.post(api_url, data=data_to_upload, headers=headers)
    #     response.raise_for_status()  # This will raise an error for bad responses (4xx or 5xx)
    #     print("Successfully uploaded data to REST API.")
    #     return True
    # except Exception as e:
    #     print(f"ERROR: Failed to upload to REST API. {e}")
    #     return False

    # --- Example 2: Upload to AWS S3 ---
    # try:
    #     import boto3
    #     s3_client = boto3.client(
    #         's3',
    #         aws_access_key_id='YOUR_AWS_ACCESS_KEY',
    #         aws_secret_access_key='YOUR_AWS_SECRET_KEY',
    #         region_name='YOUR_AWS_REGION'
    #     )
    #     bucket_name = 'your-s3-bucket-name'
    #     # Create a unique filename for each upload batch
    #     file_name = f"health_data_{int(time.time())}.json"
    #     s3_client.put_object(Bucket=bucket_name, Key=file_name, Body=data_to_upload)
    #     print(f"Successfully uploaded data to S3 bucket '{bucket_name}' as '{file_name}'.")
    #     return True
    # except Exception as e:
    #     print(f"ERROR: Failed to upload to AWS S3. {e}")
    #     return False

    # --- Example 3: Upload to Google Cloud Storage ---
    # try:
    #     from google.cloud import storage
    #     # Ensure you have set up authentication (e.g., GOOGLE_APPLICATION_CREDENTIALS env var)
    #     storage_client = storage.Client()
    #     bucket_name = 'your-gcs-bucket-name'
    #     bucket = storage_client.bucket(bucket_name)
    #     # Create a unique filename for each upload batch
    #     file_name = f"health_data_{int(time.time())}.json"
    #     blob = bucket.blob(file_name)
    #     blob.upload_from_string(data_to_upload, content_type='application/json')
    #     print(f"Successfully uploaded data to GCS bucket '{bucket_name}' as '{file_name}'.")
    #     return True
    # except Exception as e:
    #     print(f"ERROR: Failed to upload to Google Cloud Storage. {e}")
    #     return False

    # --- Default Behavior: If no example is uncommented, simulate success ---
    print("SIMULATION: No cloud provider configured. Simulating a successful upload.")
    print("Data that would be uploaded:")
    print(data_to_upload)
    return True

def main():
    """Main loop to run the sync process."""
    print("--- Starting Cloud Sync Service ---")
    
    if not os.path.exists(DB_FILE):
        print(f"ERROR: Database file '{DB_FILE}' not found. Please ensure falcon_monitor.py is running.")
        return
        
    # Ensure the database table is correctly set up for syncing
    add_sync_column_if_not_exists()
    
    while True:
        records_to_sync = fetch_unsynced_data()
        
        if records_to_sync:
            # Attempt to upload the data
            upload_successful = upload_data_to_cloud(records_to_sync)
            
            # If upload was successful, update the local DB
            if upload_successful:
                mark_data_as_synced(records_to_sync)
        
        print(f"Next check in {SYNC_INTERVAL / 60} minutes.")
        time.sleep(SYNC_INTERVAL)

if __name__ == "__main__":
    main()
